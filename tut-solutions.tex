\documentclass[12pt]{article}
\usepackage{amsmath, amssymb, amsfonts, amsthm, mathtools,mathrsfs}
\usepackage{thmtools}
\usepackage[utf8]{inputenc}
\usepackage[inline]{enumitem}
\usepackage[colorlinks=true]{hyperref}
\usepackage{tikz}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{arrows.meta}
\usepackage{witharrows}
\usepackage{datetime2}

\setlength\parindent{0pt}

\theoremstyle{definition}
\newtheorem{thm}{Theorem}
\numberwithin{thm}{section}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{defn}[thm]{Definition}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{ex}{Example}


\let\emptyset\varnothing

\pagestyle{plain}

\usepackage{titlesec}
\titleformat{\section}[block]{\sffamily\Large\filcenter\bfseries}{\S\thesection.}{0.25cm}{\Large}
\titleformat{\subsection}[block]{\large\bfseries\sffamily}{\S\S\thesubsection.}{0.2cm}{\large}

\usepackage[a4paper]{geometry}
\usepackage{lipsum}

\usepackage{cleveref}
\crefname{thm}{Theorem}{Theorems}
\crefname{lem}{Lemma}{Lemmas}
\crefname{defn}{Definition}{Definitions}
\crefname{prop}{Proposition}{Propositions}
\crefname{cor}{Corollary}{Corollaries}
\crefname{equation}{}{}

\usepackage{mdframed}
\newenvironment{blockquote}
{\begin{mdframed}[skipabove=0pt, skipbelow=0pt, innertopmargin=4pt, innerbottommargin=4pt, bottomline=false,topline=false,rightline=false, linewidth=2pt]}
{\end{mdframed}}
\newenvironment{soln}{\begin{proof}[Solution]}{\end{proof}}

\usepackage{fancyhdr}
\setlength{\headheight}{15.2pt}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\sffamily{\S\textbf{\nouppercase{\leftmark}}}}
\fancyhead[R]{\sffamily{\thepage}}
\definecolor{myupdatecolor}{RGB}{0, 0, 255}

% \usepackage{xcolor}
% \definecolor{mybgcolor}{RGB}{50, 50, 50} %46, 51, 63
% \usepackage{pagecolor}
% \pagecolor{mybgcolor}
% \color{white}
% \mdfsetup{backgroundcolor=mybgcolor, fontcolor=white}
% \definecolor{myupdatecolor}{RGB}{0, 255, 0}

\renewcommand{\familydefault}{\sfdefault}

\title{MA 205: Complex Analysis\\\large{Tutorial Solutions}}
\author{Aryaman Maithani\\\url{https://aryamanmaithani.github.io/tuts/ma-205}}
\date{Autumn Semester 2020-21\\~\\Last update: \DTMnow}

\begin{document}
\tikzset{lab dis/.store in=\LabDis,
  lab dis=0.4,
  ->-/.style args={at #1 with label #2}{decoration={
    markings,
    mark=at position #1 with {\arrow{>}; \node at (0,\LabDis) {#2};}},postaction={decorate}},
  -<-/.style args={at #1 with label #2}{decoration={
    markings,
    mark=at position #1 with {\arrow{<}; \node at (0,\LabDis)
    {#2};}},postaction={decorate}},
  -*-/.style args={at #1 with label #2}{decoration={
    markings,
    mark=at position #1 with {{\fill (0,0) circle (1.5pt);} \node at (0,\LabDis)
    {#2};}},postaction={decorate}},
  }
\maketitle
\tableofcontents
\newpage
\setcounter{section}{-1}
\section{Notations}
\begin{enumerate}
	\item Given $z \in \mathbb{C},$ $\Re z$ and $\Im z$ will denote the real and imaginary parts of $z,$ respectively.
	\item Given $z \in \mathbb{C},$ $\bar{z}$ will denote the complex conjugate of $z.$
	\item Given $z \in \mathbb{C},$ $\left|z\right|$ will denote the modulus of $z,$ defined as $\sqrt{z\bar{z}}$ or $\sqrt{\left(\Re z\right)^2 + \left(\Im z\right)^2}.$
	\item Given $z_0 \in \mathbb{C}$ and $\delta > 0,$
	\begin{equation*} 
		B_\delta(z_0) \vcentcolon= \{z \in \mathbb{C} : |z - z_0| < \delta\}.
	\end{equation*}
	\item $\mathbb{C}^\times \vcentcolon= \mathbb{C}\setminus\{0\},$ the set of nonzero complex numbers.
\end{enumerate}
\newpage\section{Tutorial 1}
\begin{center}
	25th August, 2020
\end{center}
\textbf{Notation:} The set $\mathbb{C}[x]$ is the set of all polynomials (with indeterminate $x$) with complex coefficients. Similarly, $\mathbb{R}[x]$ is defined.
\begin{enumerate}
	\item  Show that complex polynomial of degree $n$ has exactly $n$ roots. (Assuming fundamental theorem of algebra.)\\
	Remark (my own): The above is counting the roots \emph{with} multiplicity. That is, if $f(z) = (z - \iota)^2(z - 2),$ then $\iota$ is counted twice and $2$ once.
	\begin{soln}
		Let $f(x) \in \mathbb{C}[x]$ be a polynomial of degree $n.$
		We prove this via induction on $n.$\\
		$n = 1.$ Then, $f(x) = a_0 + a_1x$ for some $a_0, a_1 \in \mathbb{C}$ and $a_1 \neq 0.$\\
		Note that
		\begin{align*} 
			f(x) &= 0\\
			\iff a_0 + a_1x &= 0\\
			\iff a_1x &= -a_0\\
			\iff x &= -\dfrac{a_0}{a_1}.
		\end{align*}
		Thus, $f(x)$ has exactly $1$ root.\\~\\
		Let us assume that whenever $g(x) \in \mathbb{C}[x]$ is a polynomial of degree $n,$ then $g(x)$ has exactly $n$ roots. (Counted with multiplicity.)\\
		Let $f(x) \in \mathbb{C}[x]$ be a polynomial of degree $n + 1.$ By FTA, there exists a root $x_0 \in \mathbb{C}.$ Thus, we can write
		\begin{equation*} 
			f(x) = (x - x_0)g(x)
		\end{equation*}
		for some polynomial $g(x) \in \mathbb{C}[x]$ of degree $n.$ Moreover, note that 
		\begin{equation*} 
			f(x) = 0 \iff x = x_0 \text{ or } g(x) = 0.
		\end{equation*}
		By induction, the latter is possible for exactly $n$ values of $x.$ Thus, in total, $f(x)$ has $n + 1$ roots. (Both counts are with multiplicity.)
	\end{soln}
	%
	%
	%
	\item Show that a real polynomial that is irreducible has degree at most two. i.e., if
	\begin{equation*} 
		f(x) = a_0 + a_1x + \cdots + a_nx^n, \quad a_i \in \mathbb{R}
	\end{equation*}
	then there are non-constant real polynomials $g$ and $h$ such that $f(x) = g(x)h(x)$ if $n \ge 3.$\\
	Remark (my own): $a_n \neq 0,$ of course.
	\begin{soln}
		Let $f(x) \in \mathbb{R}[x]$ with degree $\ge 3$ as above.\\
		If $f(x)$ has a real root, then we are done by factoring as in the earlier question.\\~\\
		Thus, let us assume that $f(x) = 0$ has no real solution.\\
		We may view $f(x) \in \mathbb{C}[x].$ Now, using FTA, we know that $f(x)$ has a complex root $x_0 \in \mathbb{C}.$ By assumption, we must have $x_0 \notin \mathbb{R}$ or that $x_0 \neq \overline{x_0}.$ \\
		\begin{blockquote}
			\textbf{Claim.} $f(\overline{x_0}) = 0.$
			\begin{proof} 
				Note that

				\[\begin{WithArrows}[displaystyle]
			    f(\overline{x_0}) &= a_0 + a_1\overline{x_0} + \cdots + a_n(\overline{x_0})^n \Arrow{$\because \overline{z^n} = \bar{z}^n$}\\
					&= a_0 + a_1\overline{x_0} + \cdots + a_n\overline{x_0^n} \Arrow{$\because a_i \in \mathbb{R}$ and so, $a_i = \overline{a_i}$}\\
					&= \overline{a_0} + \overline{a_1}\;\overline{x_0} + \cdots + \overline{a_n}\overline{x_0^n} \Arrow{$\overline{z_1z_2 + z_3} = \overline{z_1}\;\overline{z_2} + \overline{z_3}$}\\
					&= \overline{f(x_0)}\\
					&= \bar{0}\\
					&= 0
			  \end{WithArrows}\]
			\end{proof}
		\end{blockquote}
		Define $g(x) = (x - x_0)(x - \overline{x_0}).$ A priori, this is a polynomial in $\mathbb{C}[x].$ However, upon multiplication, we see that the polynomial is actually an element of $\mathbb{R}[x].$ Indeed, we have
		\begin{equation*} 
			(x - x_0)(x - \overline{x_0}) = (x^2 - (2\Re x_0)x + |x_0|^2) \in \mathbb{R}[x].	
		\end{equation*}
		By our claim, we see that $g(x)$ divides $f(x)$ in $\mathbb{C}[x].$ (Since $x_0$ and $\overline{x_0}$ are distinct, the polynomials $x - x_0$ and $x - \overline{x_0}$ are ``coprime'' and thus, if they individually divide $f(x),$ then their product must too.) \\
		Thus,
		\begin{equation*} 
			f(x) = g(x)h(x)
		\end{equation*}
		for some $h(x) \in \mathbb{C}[x].$ However, since $f(x)$ and $g(x)$ are both real polynomials, so is $h(x).$ \hfill (Why?)\\
		Thus, we get that
		\begin{equation*} 
			f(x) = g(x)h(x)
		\end{equation*}
		for real polynomials $g(x)$ and $h(x).$ Moreover, note that $\deg g(x) = 2$ and $\deg h(x) = n - 2 \ge 1.$ Thus, both are non-constant.
	\end{soln}
	%\\
	%\\
	%
	\item Show that if $U$ is a path connected open set in $\mathbb{C},$ so is $U$ minus any finite set.
	%
	\begin{soln}
		We will first prove the following claim:
		\begin{blockquote}
			\textbf{Claim:} Let $U \subset \mathbb{C}$ be open and $w \in U.$ Then, $U \setminus \{w\}$ is open.
			\begin{proof} 
				Let $z_0 \in U\setminus\{w\}$ be arbitrary. Since $U$ was open, there exists $\delta_1 > 0$ such that
				\begin{equation*} 
					B_{\delta_1}(z_0) \subset U.
				\end{equation*}
				Since $z_0 \neq w,$ we have that $\delta_2 \vcentcolon= |z_0 - w| > 0.$\\
				Choose $\delta := \min\{\delta_1, \delta_2\}.$ Clearly, $\delta > 0.$ Moreover, we have
				\begin{equation*} 
					w \notin B_{\delta_2}(z_0) \supset B_{\delta}(z_0)
				\end{equation*}
				and thus, $w \notin B_{\delta}(z_0).$ Also,
				\begin{equation*} 
					B_{\delta}(z_0) \subset B_{\delta_1}(z_0) \subset U.
				\end{equation*}
				Thus, we get that
				\begin{equation*} 
					B_{\delta}(z_0) \subset U \setminus \{w\},
				\end{equation*}
				proving that $U\setminus\{w\}$ is open.
			\end{proof}
		\end{blockquote}
		By the above proof, we see that removing one point from an open set keeps it open. Thus, if we show that removing one point from an open path-connected set leaves it path-connected, then we are done since we can induct to get any other \textbf{finite}\footnote{Finiteness is important. Induction cannot prove this result for a countably infinite set.} set.\\~\\
		Thus, we now show that if $U$ is open and path-connected, so is $U\setminus\{w\}.$ (Where $w \in U$ is any arbitrary element.)\\~\\
		Let $z_0, z_1 \in U\setminus\{w\}.$ We wish to show that there is a path in $U\setminus\{w\}$ connecting $z_0$ to $z_1.$\\
		Since $U$ was path-connected to begin with, there exists a path $\sigma:[0, 1] \to U$ such that
		\begin{equation*} 
			\sigma(0) = z_0, \quad \sigma(1) = z_1.
		\end{equation*}
		If $\sigma(x) \neq w$ for any $x \in [0, 1],$ then we are done since $\sigma$ is a path in $U\setminus\{w\}$ as well.\\
		Suppose that this is not the case.\\
		Then, we choose a $\delta > 0$ such that the \emph{closed} ball
		\begin{equation*} 
			B \vcentcolon= \{z \in \mathbb{C} : |z - w| \le \delta\}
		\end{equation*}
		has the following properties:
		\begin{enumerate}
			\item $z_0 \notin B,$
			\item $z_1 \notin B,$
			\item $B \subset U.$
		\end{enumerate}
		(Why must such a $\delta$ exist? There exists a $\delta_1$ for which we get the first two properties since $z_0$ and $z_1$ are distinct from $w.$ For the last property, let $\delta_2$ be any such that $B_{\delta_2}(w) \subset U,$ which exists since $U$ is open. Then, consider $\delta_2/2.$ The \emph{closed} ball of this radius must again be completely within $U.$ Take the minimum of $\delta_1$ and $\delta_2/2$.)\\~\\
		Note that
		\begin{equation*} 
			\sigma^{-1}(B) = \{x \in [0, 1] : \sigma(x) \in B\}
		\end{equation*}
		is nonempty since $w \in B$ and $\sigma(c) = w$ for some $c \in [0, 1],$ by our assumption. \\
		Moreover, $\sigma^{-1}(B)$ must be closed. (Why?)\\
		Since it is a subset of $[0, 1],$ it is clearly bounded. Define
		\begin{equation*} 
			s \vcentcolon= \inf \sigma^{-1}(B), \quad t \vcentcolon= \sup\sigma^{-1}(B).
		\end{equation*}
		Since the set is closed, both $s$ and $t$ are elements of $\sigma^{-1}(B).$ Note that $\sigma(0) \notin B$ and $\sigma(1) \notin B$ and thus,
		\begin{equation*} 
			0 < s < t < 1.
		\end{equation*}
		(Why is the inequality $s < t$ strict?)\\
		Note that $\sigma(s)$ and $\sigma(t)$ must lie on the circumference of $B.$ (Why?) (This also shows why $s < t.$)\\
		Now consider the path $\sigma':[0, 1] \to U$ defined as follows:
		\begin{equation*} 
			\sigma'(x) = \begin{cases}
				\sigma(x) & \text{if } x \in [0, s] \cup [t, 1]\\
				\gamma(x) & \text{if } x \in [s, t],
			\end{cases}
		\end{equation*}	
		where $\gamma:[s, t] \to B$ is the path which is the arc joining $\sigma(s)$ to $\sigma(t).$ (Note that $\sigma(s) = \sigma(t)$ is possible in which case, it's the constant path.)\\
		Clearly, $\sigma'$ avoids $w$ and is continuous. \hfill (Why?)\\~\\
		Moreover, $\sigma'(0) = \sigma(0) = z_0$ and $\sigma'(1) = \sigma(1) = z_1$ and thus, $\sigma'$ is a path from $z_0$ to $z_1$ in $U \setminus \{w\},$ showing that $U\setminus\{w\}$ is path-connected.
	\end{soln}
	%\\
	%\\
	%\\
	%
	\item Check for real differentiability and holomorphicity:
	\begin{enumerate}
		\item $f(z) = c,$
		\item $f(z) = z,$
		\item $f(z) = z^n,$ $n \in \mathbb{Z},$
		\item $f(z) = \Re z,$
		\item $f(z) = \left|z\right|,$
		\item $f(z) = \left|z\right|^2,$
		\item $f(z) = \bar{z},$
		\item $f(z) = \begin{cases}
		\dfrac{z}{\bar{z}} & \text{if } z \neq 0,\\
		0 & \text{if } z = 0.
		\end{cases}$
	\end{enumerate}
	%
	\begin{soln}
		Not going to do all.
		\begin{enumerate}
			\item Real differentiable and holomorphic, both.
			\item Real differentiable and holomorphic, both.
			\item For $n \ge 0:$\\
			Real differentiable and holomorphic, both. Let us see why.\\
			As we know, holomorphicity implies real differentiability, so we only check that $f$ is holomorphic on $\mathbb{C}.$\\
			Let $z_0 \in \mathbb{C}$ be arbitrary. We show that the limit
			\begin{equation*} 
				\lim_{z\to z_0}\dfrac{f(z) - f(z_0)}{z - z_0}
			\end{equation*}
			exists.\\
			This is clear because for $z_0 \neq z,$ we have
			\begin{equation*} 
				\dfrac{z^n - z_0^n}{z - z_0} = \sum_{k=0}^{n-1}z^kz_0^{n - 1 - k}.
			\end{equation*}
			The limit $z \longrightarrow z_0$ of the RHS clearly exists.\\~\\
			$n < 0:$ The function is now defined on $\mathbb{C}\setminus\{0\}.$ It is still holomorphic and real differentiable everywhere (in its domain!).\\
			To see this, we just use the quotient rule and appeal to the previous case of $n \ge 0.$
			\item Real differentiable but not holomorphic. Note that $f$ can be written as
			\begin{equation*} 
				f(x + \iota y) = x + 0\iota.
			\end{equation*}
			Thus, $u(x, y) = x$ and $v(x, y) = 0.$\\
			This is clearly real differentiable everywhere since all the partial derivatives exist everywhere and are continuous.\\
			However, we show that $f$ is not complex differentiable at any point. Thus, it is not holomorphic.\\
			This is easy because one sees that $u_x(x_0, y_0) = 1$ and $v_y(x_0, y_0) = 0$ for all $(x_0, y_0) \in \mathbb{R}^2$ and thus, the CR equations don't hold.
			\item $|z|$ is real differentiable everywhere except $0$ and complex differentiable nowhere. Breaking the function as earlier, we have
			\begin{equation*} 
				u(x, y) = \sqrt{x^2 + y^2}, \quad v(x, y) = 0.
			\end{equation*}
			On $\mathbb{R}^2\setminus\{(0, 0)\},$ all partial derivatives exist and are continuous. At $(0, 0),$ $u_x$ and $u_y$ fail to exist.\\~\\
			This clearly shows that $f$ is not complex differentiable at $0 \in \mathbb{C}$ since it is not even real differentiable there.\\
			However, we see that $v_y = 0 = v_x$ everywhere else but at least one of $u_x$ or $u_y$ is nonzero on $\mathbb{R}^2\setminus\{(0, 0)\}$ and thus, the CR equations prevent $f$ from being complex differentiable anywhere else.
			%
			\item Real differentiable everywhere.\\
			Complex differentiable precisely at $0.$\\
			Holomorphic nowhere.\\~\\
			Same steps as above.
			%
			\item Real differentiable everywhere. Complex differentiable nowhere. Use CR equations again.
			%
			\item 
			$f$ is real differentiable precisely on $\mathbb{R}^2\setminus\{(0, 0)\}.$\\
			However, it is not complex differentiable anywhere.\\~\\
			Breaking as earlier, we get
			\begin{equation*} 
				u(x, y) = \dfrac{x^2 - y^2}{x^2 + y^2}, \quad v(x, y) = \dfrac{2xy}{x^2 + y^2},
			\end{equation*}
			for $(x, y) \in \mathbb{R}^2\setminus\{(0, 0)\}$ and
			\begin{equation*} 
				u(0, 0) = 0 = v(0, 0).
			\end{equation*}
			Note that $u$ and $v$ aren't even continuous at $(0, 0).$ Thus, neither if $f.$ Hence, $f$ is neither real nor complex differentiable at $(0, 0).$ \\
			However, at all other points, all partial derivatives exist and are continuous. Thus, $f$ is real differentiable at all those points. However, computing $u_x, u_y, v_x, v_y$ explicitly shows that the CR equations are not satisfied anywhere. Thus, $f$ is not complex differentiable anywhere. \qedhere
		\end{enumerate}
	\end{soln}
	%
	%
	%
	\item  Show that the CR equations take the form
	\begin{equation*} 
		u_r = \dfrac{1}{r}v_\theta, \quad v_r = -\dfrac{1}{r}u_\theta
	\end{equation*}
	in polar coordinates.
	%\\
	\begin{soln}
		We shall follow the same idea as in the slides. We first write
		\begin{equation*} 
			f(r, \theta) = f(re^{\iota\theta}) = u(r, \theta) + \iota v(r, \theta).
		\end{equation*}
		Suppose that $f$ is differentiable at $z_0 = r_0e^{\iota\theta_0} \neq 0.$ (Note that it wouldn't make sense to talk at $0$ since there's a $r^{-1}$ factor in the question anyway.)\\
		Thus, we know that the limit
		\begin{equation*} 
			\lim_{z\to z_0}\dfrac{f(z) - f(z_0)}{z - z_0}
		\end{equation*}
		exists. We shall calculate it in two ways:
		\begin{enumerate}
			\item Fix $\theta = \theta_0$ and let $r \to r_0.$ Then, we get
			\begin{align*} 
				f'(z_0) &= \lim_{r\to r_0}\left\{\dfrac{u(r, \theta_0) - u(r_0, \theta_0)}{e^{\iota\theta_0}(r - r_0)} + \iota\dfrac{v(r, \theta_0) - v(r_0, \theta_0)}{e^{\iota\theta_0}(r - r_0)}\right\}\\~\\
				&= e^{-\iota\theta_0}\lim_{r\to r_0}\left\{\dfrac{u(r, \theta_0) - u(r_0, \theta_0)}{r - r_0} + \iota\dfrac{v(r, \theta_0) - v(r_0, \theta_0)}{r - r_0}\right\}\\~\\
				&= e^{-\iota\theta_0}\left(u_r(r_0, \theta_0) + \iota v_r(r_0, \theta_0)\right). & (*)
			\end{align*}

		\item Fix $r = r_0$ and let $\theta \to \theta_0.$ Then, we get
		\begin{align*} 
			f'(z_0) &= \lim_{\theta\to \theta_0}\left\{\dfrac{u(r_0, \theta) - u(r_0, \theta_0)}{r_0(e^{\iota\theta} - e^{\iota\theta_0})} + \iota\dfrac{v(r_0, \theta) - v(r_0, \theta_0)}{r_0(e^{\iota\theta} - e^{\iota\theta_0})}\right\}\\~\\
			&= \dfrac{1}{r_0}\lim_{\theta\to \theta_0}\left\{\dfrac{u(r_0, \theta) - u(r_0, \theta_0)}{e^{\iota\theta} - e^{\iota\theta_0}} + \iota\dfrac{v(r_0, \theta) - v(r_0, \theta_0)}{e^{\iota\theta} - e^{\iota\theta_0}}\right\} & (**)
		\end{align*}
		We concentrate on the first term of the limit. Note that
		\begin{align*} 
			&\lim_{\theta\to \theta_0}\dfrac{u(r_0, \theta) - u(r_0, \theta_0)}{e^{\iota\theta} - e^{\iota\theta_0}}\\~\\
			=& \lim_{\theta\to \theta_0}\dfrac{u(r_0, \theta) - u(r_0, \theta_0)}{\theta - \theta_0}\dfrac{\theta - \theta_0}{e^{\iota\theta} - e^{\iota\theta_0}}.
		\end{align*}
		In the product, the first term is clearly $u_\theta(r_0, \theta_0),$ after taking the limit. The second term can be calculated to be
		\begin{equation*} 
			\dfrac{1}{\iota e^{\iota\theta_0}}.
		\end{equation*}
		(How? Write $e^{\iota\theta}$ in terms of $\cos$ and $\sin$ and differentiate those and put it back.)\\
		Of course, a similar argument goes through for the $v$ term as well.\\
		Thus, we get that $(**)$ transforms to
		\begin{equation*} 
			f'(z_0) = \dfrac{e^{-\iota\theta_0}}{r_0}\left(\iota u_\theta(r_0, \theta_0) + v_\theta(r_0, \theta_0)\right).
		\end{equation*}
		\end{enumerate}
		Equating the above with $(*),$ cancelling $e^{-\iota\theta_0},$ and comparing the real and imaginary parts, we get
		\begin{equation*} 
			u_r(r_0, \theta_0) = \dfrac{1}{r_0}v_\theta(r_0, \theta_0), \quad v_r(r_0, \theta_0) = -\dfrac{1}{r_0}u_\theta(r_0, \theta_0),
		\end{equation*}
		as desired.
	\end{soln}
\end{enumerate}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%

\newpage\section{Tutorial 2}
\begin{center}
	1st September, 2020
\end{center}
\begin{enumerate}
	\item  If $u(X,Y)$ and $v(X,Y)$ are harmonic conjugates of each other, show that they are constant functions.\\
	Remark (my own): This is true iff $u$ and $v$ are defined on domains, that is, open and path-connected sets.
	\begin{soln}
		Since $v$ is a harmonic conjugate of $u,$ we get that
		\begin{equation*} 
			u_X = v_Y, \quad u_Y = -v_X.
		\end{equation*}
		On the other hand, since $u$ is a harmonic conjugate of $v,$ we get that
		\begin{equation*} 
			v_X = u_Y, \quad v_Y = -u_X.
		\end{equation*}
		(Note that the equalities mean that they're true for every $(X_0, Y_0)$ in the domain.)\\
		Thus, we get that
		\begin{equation*} 
			u_X = u_Y = v_X = v_Y \equiv 0,
		\end{equation*}
		identically. \\
		Since the domain is connected, this implies that $u$ and $v$ are constant.
	\end{soln}
	%
	\item Show that $u = XY - 3X^2Y - Y^3$ is harmonic and find its harmonic conjugate.
	\begin{soln} \phantom{hi}\\	
		\textbf{Smart way:} If we can show that the above function is the real (or imaginary) part of a holomorphic function $f,$ then we have shown that $u$ is harmonic.\\
		Writing $Z = X + iY,$ it is not too tough to see that the above is the \textbf{imaginary} part of $\frac{1}{2}Z^2 + Z^3.$ Since
		\begin{equation*} 
			f(Z) = \dfrac{1}{2}Z^2 + Z^3
		\end{equation*}
		is holomorphic on $\mathbb{C},$ this gives us that $u$ is harmonic.\\
		This also shows \emph{a} harmonic conjugate of $u$ is 
		\begin{equation*} 
			v(X, Y) = -\Re f(Z) = \dfrac{1}{2}(Y^2 - X^2) + 3XY^2 - X^3.
		\end{equation*}
		(Note the negative sign! If we had gotten $u$ as the \emph{real} part of a holomorphic function, then for finding harmonic conjugate, we would've simply taken the imaginary part \emph{without} the negative sign.)

		\dotfill

		\textbf{Laborious way:} This is the way to do it if observing is difficult.\\
		First, we show that $u$ is harmonic by manual calculation. Note that
		\begin{equation*} 
			u_{XX}(X_0, Y_0) = 6Y_0 \text{ and } u_{YY}(X_0, Y_0) = -6Y_0.
		\end{equation*} 
		Thus, $u_{XX} + u_{YY} \equiv 0$ and $u$ is indeed harmonic.\\~\\
		To find its harmonic conjugate, we perform the procedure as given in slides.\\
		Note that $u_X = v_Y.$ Here, we get $u_X = Y + 6XY = v_Y.$\\
		Integrating with respect to $Y$ gives us
		\begin{equation*} 
			v = \dfrac{1}{2}Y^2 + 3XY^2 + g(X)
		\end{equation*}
		for some function $g.$ Then, we need $v_X = -u_Y.$ Computing each individually, we get
		\begin{equation*} 
			3Y^2 + g'(X) = -X - 3X^2 + 3Y^2.
		\end{equation*}
		Thus, up to a constant, we get
		\begin{equation*} 
			g(X) = -\dfrac{1}{2}X^2 - X^3.
		\end{equation*}
		Finally, this gives
		\begin{equation*} 
			v = \dfrac{1}{2}Y^2 + 3XY^2 - \dfrac{1}{2}X^2 - X^3. \qedhere
		\end{equation*}
	\end{soln}
	%\\
	%\\
	\item Find the radius of convergence of the following power series:
	\begin{enumerate}
		\item $\displaystyle\sum_{n=0}^{\infty}nz^n,$
		\item $\displaystyle\sum_{p \text{ prime}} z^p,$
		\item $\displaystyle\sum_{n=1}^{\infty}\dfrac{n!}{n^n}z^n.$
	\end{enumerate}
	\begin{soln}
		We shall be using the root test in the first two cases and ratio test in the third. \\
		One thing to recall is that \emph{if} the limit $\displaystyle\lim_{n\to \infty}a_n$ exists, \emph{then} $\displaystyle\limsup_{n\to\infty}a_n$ is equal to that limit. This will be helpful in the first and third parts since the limits will themselves exist.\\
		Moreover, we recall that if
		\begin{equation*} 
			\alpha = \limsup_{n\to\infty}\sqrt[n]{|a_n|},
		\end{equation*}
		then the radius of convergence $R$ is given by
		\begin{equation*} 
			R = \alpha^{-1}.
		\end{equation*}
		(The case $\alpha = 0$ corresponds to $R = \infty$ and vice-versa.)\\
		Similar analysis holds for
		\begin{equation*} 
			\alpha = \lim_{n \to \infty}\left|\dfrac{a_{i+1}}{a_i}\right|.
		\end{equation*}
		(Here, however, note that I need the existence of $\alpha.$ In the case of $\limsup,$ that was always guaranteed.)
		\begin{enumerate}
			\item Note that we have
			\begin{equation*} 
				\lim_{n\to \infty}\sqrt[n]{n} = 1.
			\end{equation*}
			(MA 105 Tutorial Sheet 1, Question 2 (iv))\\~\\
			Thus, we also have
			\begin{equation*} 
				\alpha = \limsup_{n\to\infty}\sqrt[n]{n} = 1
			\end{equation*}
			and thus,
			\begin{equation*} 
				R = \alpha^{-1} = \boxed{1.}
			\end{equation*}
			\item Note that first we can rewrite the series in the form
			\begin{equation*} 
				\sum_{n=1}^{\infty}a_nz^n,
			\end{equation*}	
			where
			\begin{equation*} 
				a_n \vcentcolon= \begin{cases}
					0 & n \text{ is not a prime,}\\
					1 & n \text{ is a prime.}
				\end{cases}
			\end{equation*}
			For this, we clearly have
			\begin{equation*} 
				\limsup_{n\to\infty}\sqrt[n]{|a_n|} = \lim_{n\to \infty}1 = 1.
			\end{equation*}
			(To see this, note that there are infinitely many primes and thus, given any $n,$ there exists $m \ge n$ such that $a_m = 1.$)\\
			Thus, as before, the radius of convergence is $1.$
			\item Here, we have
			\begin{equation*} 
				a_n = \dfrac{n!}{n^n}.
			\end{equation*}
			Thus, we get
			\begin{align*} 
				\alpha = \lim_{n\to \infty}\left|\dfrac{a_{n+1}}{a_n}\right| &= \lim_{n\to \infty}(n + 1)\dfrac{n^n}{(n + 1)^{n + 1}}\\
				&= \lim_{n\to \infty}\left(1 + \dfrac{1}{n}\right)^{-n}\\
				&= e^{-1}.
			\end{align*}
			Thus, the limit actually exists and we get
			\begin{equation*} 
				R = \alpha^{-1} = \boxed{e.} \qedhere
			\end{equation*}
		\end{enumerate}
	\end{soln}
	%\\
	%\\
	%
	\item Show that $L > 1$ in the ratio test (Lecture 3 slides) does not necessarily imply that the series is divergent.
	\begin{soln}
		Consider the sequence
		\begin{equation*} 
			\dfrac{1}{1^3},\;\dfrac{1}{1^2},\;\dfrac{1}{2^3},\;\dfrac{1}{2^2},\;\ldots,\;\dfrac{1}{n^3},\;\dfrac{1}{n^2},\;\ldots.
		\end{equation*}
		That is, let $(a_n)$ be the sequence defined by
		\begin{equation*} 
			a_{2n} = \dfrac{1}{n^2}, \quad a_{2n - 1} = \dfrac{1}{n^3}.
		\end{equation*}
		Note that $\sum a_n$ converges, since $\sum n^{-2}$ and $\sum n^{-3}$ converge. (This can be checked via the integral test.)\\
		On the other hand, note that that 
		\begin{equation*} 
			L = \limsup_{n \to \infty}\left|\dfrac{a_{n + 1}}{a_n}\right| \ge \limsup_{n \to \infty}\left|\dfrac{a_{2n}}{a_{2n - 1}}\right| = \limsup_{n \to \infty} n = \infty.
		\end{equation*}
		Thus, $L = \infty,$ clearly $> 1.$\\
		(So, not only did we show that $L > 1$ doesn't imply {\color{myupdatecolor}divergence} but also that even $L = \infty$ is not good enough to conclude divergence.)
	\end{soln}
	%
	%
	%
	\item Construct a infinitely differentiable function $f:\mathbb{R}\to\mathbb{R}$ which is non-zero but vanishes outside a bounded set. Show that there are no holomorphic functions which satisfy this property.
	%
	\begin{soln}
		Recall the function $g:\mathbb{R}\to\mathbb{R}$ from the lectures given as
		\begin{equation*} 
			g(x) \vcentcolon= \begin{cases}
				0 & x \le 0,\\
				e^{-1/x} & x > 0.
			\end{cases}
		\end{equation*}
		As we saw, this is an infinitely differentiable function. Now, consider $f:\mathbb{R} \to \mathbb{R}$ defined as
		\begin{equation*} 
			f(x) \vcentcolon= g(x)g(1 - x).
		\end{equation*}
		Clearly, $f$ is infinitely differentiable, being the product of two such functions. Moreover, $f(x) = 0$ if $x \le 0$ or $x \ge 1.$ In other words, $f$ is $0$ outside the bounded set
		\begin{equation*} 
			(0, 1).
		\end{equation*}
		However, $f$ is non-zero since
		\begin{equation*} 
			f\left(\dfrac{1}{2}\right) = \left(g\left(\dfrac{1}{2}\right)\right)^2 = e^{-4} \neq 0.
		\end{equation*}
		On the other hand, let $f:\mathbb{C}\to\mathbb{C}$ be a holomorphic function which is zero outside some bounded set $K.$ We show that $g$ is zero everywhere. \\
		Since $K$ is bounded, there exists $M > 0$ such that
		\begin{equation*} 
			|z| \le M \quad \text{ for all } z \in K.
		\end{equation*}
		Thus, choosing the point $z_0 = M + 43,$ we see that $f$ is zero in the neighbourhood of $z_0$ of radius $42.$ \hfill (Why?)\\
		However, since $\mathbb{C}$ is (open and) path-connected, this implies that $f$ is zero \emph{everywhere}, as desired.
	\end{soln}
		
		\hrulefill
		
		Some more elaboration on the last part: In the lectures, we had seen the result that if $\Omega$ is a domain and $f:\Omega \to \mathbb{C}$ is analytic, then $f$ has the following property:

		\begin{mdframed}
			Either $f$ is identically zero or the zeroes of $f$ form a discrete set.
		\end{mdframed}

		Since any open disc is not discrete, we get that

		\begin{mdframed}
			\begin{equation*} 
				f \text{ is zero on a neighbourhood } \implies f \text{ is zero everywhere on }\Omega.
			\end{equation*}
		\end{mdframed}

		However, note that we had proved the result for analytic functions. As we shall see later in the course, holomorphic functions are indeed analytic.
	%\\
	%\\
	%
	\item Show that $\exp:\mathbb{C}\to\mathbb{C}^\times$ is onto.
	\begin{soln}
		Let $z_0 \in \mathbb{C}^\times.$ We show that $\exp(z) = z_0$ for some $z \in \mathbb{C}.$ \\
		Note that $r = |z_0| \neq 0.$\\
		Then,
		\begin{equation*} 
			w = \dfrac{z_0}{r}
		\end{equation*}
		has modulus $1.$ In other words,
		\begin{equation*} 
			w = x_0 + \iota y_0
		\end{equation*}
		for some $(x_0, y_0) \in \mathbb{R}^2$ such that $x_0^2 + y_0^2 = 1.$\\
		Thus, $x_0 = \cos\theta$ and $y_0 = \sin\theta$ for some $\theta \in [0, 2\pi).$\\~\\
		Define $z = \log(r) + \iota\theta.$ Note that this $\log$ is the real-valued $\log.$ Thus, we get
		\begin{align*} 
			\exp(z) = \exp(\log(r) + \iota\theta) &= \exp(\log(r))\cdot\exp(\iota\theta)\\
			&= r\cdot(\cos\theta + \iota\sin\theta)\\
			&= rw = z_0.
		\end{align*}
		Thus, $\exp$ is surjective.
	\end{soln}

	%\\
	%\\
	%\\
	\item Show that $\sin, \cos: \mathbb{C}\to\mathbb{C}$ are surjective. (In particular, note the difference with real sine and cosine which were bounded by $1$).
	%
	\begin{soln}
		We show this for $\cos.$ The method works the same for $\sin.$\\
		Recall that
		\begin{equation*} 
			\cos(z) = \dfrac{1}{2}\left(e^{\iota z} + e^{-\iota z}\right).
		\end{equation*}
		Let $z_0 \in \mathbb{C}.$ We show that $\cos(z) = z_0$ for some $z \in \mathbb{C}.$\\
		Consider the quadratic equation
		\begin{equation*} 
			\dfrac{1}{2}\left(t + \dfrac{1}{t}\right) = z_0. \quad (*)
		\end{equation*}
		Rearranging this gives
		\begin{equation*} 
			t^2 - 2z_0t + 1 = 0.
		\end{equation*}
		Note that the above has complex solutions $t_1$ and $t_2.$ (Since every complex number has a square root in $\mathbb{C}$!)\\
		Moreover, note that $t_1 \neq 0.$ Thus, by the previous part, there exists $z \in \mathbb{C}$ such that $e^z = t_1.$ {\color{myupdatecolor}Considering $z' = z/\iota,$ we get that $e^z = e^{\iota z'} = t_1.$ \\
				Plugging $t_1 = e^{\iota z'}$ in $(*)$ shows that
				\begin{equation*} 
					\cos(z') = z_0,
				\end{equation*}
				as desired.	}
	\end{soln}
	%
	%
	\item Show that for any complex number $z,$ $\sin^2(z) + \cos^2(z) = 1.$
	\begin{soln}
		Recall the definitions
		\begin{equation*} 
			\iota\sin(z) = \dfrac{1}{2}\left(e^{\iota z} - e^{-\iota z}\right), \quad \cos(z) = \dfrac{1}{2}\left(e^{\iota z} + e^{-\iota z}\right).
		\end{equation*}
		Squaring and subtracting gives
		\begin{equation*} 
			(\cos(z))^2 - (\iota\sin(z))^2 = \dfrac{1}{4}(4e^{\iota z}e^{-\iota z}) = 1
		\end{equation*}
		or
		\begin{equation*} 
			\sin^2(z) + \cos^2(z) = 1.
		\end{equation*}

		\dotfill

		{\color{myupdatecolor}\textbf{Smarter way:} Consider the function $f:\mathbb{C}\to\mathbb{C}$ defined as 
				\begin{equation*} 
					f(z) = \cos^2z + \sin^2z - 1.
				\end{equation*}
				This is analytic and vanishes on $\mathbb{R}.$ Since $\mathbb{R}$ is not discrete, it must vanish everywhere.}
	\end{soln}
\end{enumerate}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%

\newpage\section{Tutorial 3}

\begin{enumerate}
	\item Let $\gamma$ be the boundary of the triangle
	\begin{equation*} 
		\{0 < y < 1 - x; 0 \le x \le 1\}
	\end{equation*}
	taken with the anticlockwise orientation. 

	\begin{center}
		\begin{tikzpicture}
			\def \len{3};
			\def \del{0.3};
			\draw[thick, -<-=at 0.5 with label {$\gamma_1$}](\len, 0) -- (0, 0);
			\draw[thick, -<-=at 0.5 with label {$\gamma_2$}](0, \len) -- (\len, 0);
			\draw[thick, -<-=at 0.5 with label {$\gamma_3$}](0, 0) -- (0, \len);
			\node[] at (-\del, -\del) {$(0, 0)$};
			\node[] at (\len + \del, -\del) {$(1, 0)$};
			\node[] at (0, \len + \del) {$(0, 1)$};
		\end{tikzpicture}
	\end{center}

	Evaluate:
	\begin{enumerate}
		\item $\displaystyle\int_\gamma\Re(z)dz$\\~\\
		Note that we can compute the integrals along $\gamma_1, \ldots, \gamma_3$ and then add them.\\
		Along $\gamma_3,$ the integral must be $0$ since $\Re(z) = 0$ along that curve.\\
		Along $\gamma_1,$ we parameterise the curve as
		\begin{equation*} 
			\gamma_1(t) = t + 0\iota, \quad \text{for } t \in [0, 1].
		\end{equation*}
		Then, we get that $\gamma_1'(t) = 1 + 0\iota.$ Thus, the integral is calculated as
		\begin{align*} 
			\int_{\gamma_1}\Re(z){\mathrm{d}}z &= \int_{0}^{1} \Re(\gamma_1(t))\gamma_1'(t) {\mathrm{d}}t\\
			&= \int_{0}^{1} t {\mathrm{d}}t\\
			&= \dfrac{1}{2}.
		\end{align*}

		Similarly, we compute the integral along $\gamma_2.$ First, we parameterise it as
		\begin{equation*} 
			\gamma_2(t) = 1 - t + \iota t, \quad \text{for } t \in [0, 1].
		\end{equation*}
		We compute the derivative as $\gamma_2'(t) = -1 + \iota.$ Thus, the integral is calculated as
		\begin{align*} 
			\int_{\gamma_2}\Re(z){\mathrm{d}}z &= \int_{0}^{1} \Re(\gamma_2(t))\gamma_2'(t) {\mathrm{d}}t\\
			&= \int_{0}^{1} (1 - t)(-1 + \iota) {\mathrm{d}}t\\
			&= \dfrac{1}{2}(\iota - 1).
		\end{align*}
		Thus, we get the overall integral as
		\begin{equation*} 
			\int_{\gamma}^{} \Re(z) {\mathrm{d}}z = \boxed{\dfrac{\iota}{2}}.
		\end{equation*}
		\item $\displaystyle\int_{\gamma}^{} z^2 {\mathrm{d}}z.$\\~\\
		Note that $\gamma$ is a closed curve and $z^2$ admits a primitive on $\mathbb{C}.$ Thus, we get that the integral is $\boxed{0}.$
	\end{enumerate}
	%	
	\item Compute $\displaystyle\int_{|z - 1| = 1}^{} \dfrac{2z - 1}{z^2 - 1} {\mathrm{d}}z.$

	Remark (my own): If nothing is specified, we assume that the integral is in the counterclockwise sense.

	\begin{soln}
		Note that the curve of integration does not enclose $-1.$ Keeping this in mind, we define
		\begin{equation*} 
			f:\mathbb{C}\setminus\{-1\} \to \mathbb{C}
		\end{equation*}	
		as 
		\begin{equation*} 
			f(z) = \dfrac{2z - 1}{z + 1}.
		\end{equation*}
		Note that this is holomorphic on $\Omega = \mathbb{C}\setminus\{-1\}.$ Moreover, $\gamma$ and its interior lie completely within $\Omega.$ Thus, using the Cauchy integral formula, we see that (assuming the circle is traverses counterclockwise)
		\begin{equation*} 
			2\pi\iota f(1) = \int_{|z - 1| = 1}^{} \dfrac{f(z)}{z - 1} {\mathrm{d}}z.
		\end{equation*}
		However, note that the integral on the right is precisely what we wish to calculate. Thus, we get the desired integral's value as
		\begin{equation*} 
			2\pi\iota f(1) = \boxed{\pi\iota}. \qedhere
		\end{equation*}
	\end{soln}
	%
	\item Show that if $\gamma$ is a simple closed curve traced counterclockwise, the integral $\displaystyle\int_{\gamma}^{} \bar{z} {\mathrm{d}}z$ equals $2\iota\operatorname{Area}(\gamma).$\\
	Evaluate $\displaystyle\int_{\gamma}^{} \bar{z}^m {\mathrm{d}}z$ over a circle $\gamma$ centered at the origin.
	\begin{soln}
		Suppose that $\gamma(t) = x(t) + \iota y(t)$ for $t \in [a, b].$
		\begin{align*} 
			\displaystyle\int_{\gamma}^{} \bar{z} {\mathrm{d}}z &= \int_{a}^{b} \overline{\gamma(t)}\gamma'(t) {\mathrm{d}}t\\
			&= \int_{a}^{b} (x(t) - \iota y(t))(x'(t) + \iota y'(t)) {\mathrm{d}}t\\
			&= \int_{a}^{b} (x(t)x'(t) + y(t)y'(t)) {\mathrm{d}}t + \iota\int_{a}^{b} (x(t)y'(t) - x(t)y'(t)) {\mathrm{d}}t\\
			&= \int_{\gamma}(x{\mathrm{d}}x + y{\mathrm{d}}y) + \iota\int_{\gamma}(x{\mathrm{d}}y - y{\mathrm{d}}x)\\
			&= \iint_{\operatorname{Int}(\gamma)}(0 - 0){\mathrm{d}}(x, y) + \iota\iint_{\operatorname{Int}(\gamma)}(1 - (-1)){\mathrm{d}}(x, y)\\
			&= 2\iota\iint_{\operatorname{Int}(\gamma)}{\mathrm{d}}(x, y)\\
			&= 2\iota\operatorname{Area}(\gamma).
		\end{align*}
		In going from the single integral to the double integral, we used Green's theorem which said that
		\begin{equation*} 
			\int_{\gamma}(M{\mathrm{d}}x + N{\mathrm{d}}y) = \iint_{\operatorname{Int}(\gamma)} \left(\dfrac{\partial N}{\partial x} - \dfrac{\partial M}{\partial y}\right) {\mathrm{d}}(x, y)
		\end{equation*}
		if $\gamma$ is a (nice-enough) closed curve oriented counterclockwise. (Here is where we have used orientation.)

		For the second part, we simply parameterise the curve as
		\begin{equation*} 
			\gamma(t) = r(\cos t + \iota \sin t), \quad \text{for } t \in [0, 2\pi],
		\end{equation*}
		where $r > 0$ is arbitrary.

		We see that $\gamma'(t) = r(-\sin t + \iota\cos t) = \iota\gamma(t).$ \\
		Thus, we get

		\begin{align*} 
			\int_{\gamma}^{} \bar{z}^m {\mathrm{d}}z &= \int_{0}^{2\pi} \overline{(\gamma(t))}^m \gamma'(t) {\mathrm{d}}t\\
			&= \int_{0}^{2\pi} \overline{(\gamma(t))}^{m-1} \cdot \overline{\gamma(t)} \gamma'(t) {\mathrm{d}}t\\
			&= \int_{0}^{2\pi} \overline{(\gamma(t))}^{m-1} \cdot \overline{\gamma(t)} \iota\gamma(t) {\mathrm{d}}t\\
			&= \iota\int_{0}^{2\pi} \overline{(\gamma(t))}^{m-1} \cdot \left|\gamma(t)\right|^2 {\mathrm{d}}t\\
			&= \iota r^2\int_{0}^{2\pi} r^{m-1}(\cos((m-1)t) - \iota\sin((m-1)t)) {\mathrm{d}}t
		\end{align*}
		Note that the $\sin$ integral is $0$ regardless of $m.$ However, the $\cos$ integral is $0$ iff $m \neq 1.$ If $m = 1,$ then we get that
		\begin{equation*} 
			\int_{0}^{2\pi} \cos(0t) {\mathrm{d}}t = 2\pi.
		\end{equation*}
		Thus, we get
		\begin{equation*} 
			\int_{\gamma}^{} \bar{z}^m {\mathrm{d}}z = \begin{cases}
				2\pi\iota r^2 & m = 1,\\
				0 & m \neq 1.
			\end{cases}
		\end{equation*}
	\end{soln}
	%
	%
	\item Let $\mathbb{H} = \{z \in \mathbb{C} \mid \Re(z) > 0\}$ be the (strict) open right half plane. Construct a {\color{myupdatecolor}non-constant} function $f$ which is holomorphic on $\mathbb{H}$ such that $f\left(\frac{1}{n}\right) = 0$ for all $n \in \mathbb{N}.$\\
	Note that the {\color{myupdatecolor}coloured} part is my addition.
	\begin{soln}
		Define 
		\begin{equation*} 
			f(z) \vcentcolon= \sin\left(\dfrac{\pi}{z}\right).
		\end{equation*}
		Since $0 \notin \mathbb{H},$ we see that $f$ is a composition of holomorphic functions and hence, is holomorphic. Moreover, one see that for any $n \in \mathbb{N},$ we have
		\begin{equation*} 
			f\left(\dfrac{1}{n}\right) = \sin(n\pi) = 0,
		\end{equation*}
		as desired. Lastly, $f$ is non-constant since
		\begin{equation*} 
			f(2) = \sin\left(\dfrac{\pi}{2}\right) = 1 \neq 0. \qedhere
		\end{equation*}
	\end{soln}
	%
	\item Let $f$ be a holomorphic function on $\mathbb{C}$ such that $f\left(\frac{1}{n}\right) = 0$ for all $n \in \mathbb{N}.$ Show that $f$ is constant.
	\begin{soln}\phantom{hi}\\
		\begin{blockquote}
		Motivation: We would like to appeal to the theorem about analytic (and hence, holomorphic) functions which said that if the set of zeroes of $f$ is not discrete, then $f$ is identically zero. However, we cannot directly use that result since $\{n^{-1} : n \in \mathbb{N}\}$ is in fact, a discrete set.\\
		However, the difference here is that $0$ is in the domain and $0$ is a ``limit point'' of the above set. We shall use this to our advantage.

		It may be useful to ``recall'' the definition:

		\begin{defn}[Discrete Set]
			A set $S \subset \Omega$ is said to be \emph{discrete} if for every $s \in S,$ there exists some $\delta > 0$ such that
			\begin{equation*} 
				B_\delta(s) \cap S = \{s\}.
			\end{equation*}
			In other words, for every $s \in S,$ there exists some $\delta > 0$ such that the $\delta$ neighbourhood of $s$ contains no other point of $S.$
		\end{defn}
		\end{blockquote}

		Note that $f$ is holomorphic. In particular, $f$ is continuous. Using this we see that
		\begin{align*} 
			f(0) &= f\left(\lim_{n\to \infty}\dfrac{1}{n}\right)\\
			&=\lim_{n\to \infty}f\left(\dfrac{1}{n}\right)\\
			&= \lim_{n\to \infty}0\\
			&= 0.
		\end{align*}
		(Note that here we know that $f$ is indeed defined at $0$ and thus, the above computations are valid.)

		\emph{Now}, we see that $f$ is zero on
		\begin{equation*} 
			S = \{0\}\cup\left\{\dfrac{1}{n} : n \in \mathbb{N}\right\}.
		\end{equation*}
		(It may be zero outside $S$ as well.)

		However, we see that $S$ is \emph{not} discrete. To see this, note that $0 \in S$ and given any $\delta > 0,$ there exists $n \in \mathbb{N}$ such that $1/n < \delta.$ Thus, for any $\delta > 0,$ we have that $B_\delta \cap S$ contains a point apart from $0.$ This shows that $S$ is not discrete.

		From this, we conclude that $f$ is identically zero. In particular, it is constant.
	\end{soln}
	%
	\item Expand $\displaystyle\dfrac{1 + z}{1 + 2z^2}$ into a power series around $0.$ Find the radius of convergence.
	\begin{soln}
		First, we note that
		\begin{equation*} 
			\dfrac{1 + z}{1 + 2z^2} = \dfrac{1}{2}\cdot \dfrac{2 + 2z^2}{1 + 2z^2} = \dfrac{1}{2}\left(1 + \dfrac{1}{1 + 2z^2}\right).
		\end{equation*}
		Thus, it suffices to solve the question for
		\begin{equation*} 
			f(z) = \dfrac{1}{1 + 2z^2}
		\end{equation*}
		instead of the given function. \hfill (Why?)

		Now, one may compute the power series by computing $f^{(n)}(0).$ However, if one already knows a power series by some other means, we may directly use that. Recall in our derivations that the coefficients of the power series were independent of the radius. Thus, if we know \emph{some} power series expansion with \emph{some} radius of convergence, that must be \emph{the} power series expansion with \emph{that} radius of convergence.

		In this case, we have
		\begin{equation*} 
			\dfrac{1}{1 + 2z^2} = 1 - 2z^2 + (2z^2)^2 - (2z^2)^3 + \cdots
		\end{equation*}
		for $|2z^2| < 1$ or $|z| < \dfrac{1}{\sqrt{2}}.$

		Thus, we get the power series expansion of $f$ around $0$ to be
		\begin{equation*} 
			f(z) = 1 - 2z^2 + 4z^4 - 8z^6 + \cdots, \quad \text{for } |z| < \dfrac{1}{\sqrt{2}}.
		\end{equation*}

		Note that we do know that the series diverges for $|z| > 1/\sqrt{2}$ and thus, we may conclude that the radius of convergence is $\boxed{\dfrac{1}{\sqrt{2}}}.$
	\end{soln}
\end{enumerate}	
\end{document}